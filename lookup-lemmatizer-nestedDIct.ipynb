{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Wordform types': 16879, 'Wordform tokens': 281057, 'Unambiguous types': 16465, 'Unambiguous tokens': 196204, 'Ambiguous types': 414, 'Ambiguous tokens': 84853, 'Ambiguous most common tokens': 75667, 'Identity tokens': 201485}\n",
      "{'Total test items': 35430, 'Found in lookup table': 35430, 'Lookup match': 33855, 'Lookup mismatch': 1575, 'Not found in lookup table': 0, 'Identity match': 1227, 'Identity mismatch': 354}\n",
      "Expected lookup accuracy: 0.9673162383431119\n",
      "Expected identity accuracy: 0.7168830521922599\n",
      "Lookup accuracy: 0.9639280333244704\n",
      "Identity accuracy: 0.7760910815939279\n",
      "Overall accuracy: 0.9555461473327689\n"
     ]
    }
   ],
   "source": [
    "### This program is a very simple lemmatizer, which learns a\n",
    "### lemmatization function from an annotated corpus. The function is\n",
    "### so basic I wouldn't even consider it machine learning: it's\n",
    "### basically just a big lookup table, which maps every word form\n",
    "### attested in the training data to the most common lemma associated\n",
    "### with that form. At test time, the program checks if a form is in\n",
    "### the lookup table, and if so, it gives the associated lemma; if the\n",
    "### form is not in the lookup table, it gives the form itself as the\n",
    "### lemma (identity mapping).\n",
    "\n",
    "### The program performs training and testing in one run: it reads the\n",
    "### training data, learns the lookup table and keeps it in memory,\n",
    "### then reads the test data, runs the testing, and reports the\n",
    "### results.\n",
    "\n",
    "### The program takes two command line arguments, which are the paths\n",
    "### to the training and test files. Both files are assumed to be\n",
    "### already tokenized, in Universal Dependencies format, that is: each\n",
    "### token on a separate line, each line consisting of fields separated\n",
    "### by tab characters, with word form in the second field, and lemma\n",
    "### in the third field. Tab characters are assumed to occur only in\n",
    "### lines corresponding to tokens; other lines are ignored.\n",
    "\n",
    "import operator\n",
    "import sys\n",
    "import re\n",
    "\n",
    "### Global variables\n",
    "\n",
    "# Paths for data are read from command line\n",
    "#train_file = sys.argv[1]\n",
    "#test_file = sys.argv[2]\n",
    "train_file = './data/hi_hdtb-ud-train.conllu'\n",
    "test_file = './data/hi_hdtb-ud-test.conllu'\n",
    "\n",
    "# Counters for lemmas in the training data: word form -> lemma -> count\n",
    "lemma_count = {}\n",
    "\n",
    "# Lookup table learned from the training data: word form -> lemma\n",
    "lemma_max = {}\n",
    "\n",
    "# Varialbe for counting total tokens\n",
    "wordform_tokens = 0\n",
    "unambiguous_types, unambiguous_tokens = 0, 0\n",
    "ambiguous_types, ambiguous_tokens = 0, 0\n",
    "ambiguous_most_common_tokens = 0\n",
    "identity_tokens = 0\n",
    "\n",
    "# Variables for reporting results\n",
    "training_stats = ['Wordform types' , 'Wordform tokens' , 'Unambiguous types' , 'Unambiguous tokens' , 'Ambiguous types' , 'Ambiguous tokens' , 'Ambiguous most common tokens' , 'Identity tokens']\n",
    "training_counts = dict.fromkeys(training_stats , 0)\n",
    "\n",
    "test_outcomes = ['Total test items' , 'Found in lookup table' , 'Lookup match' , 'Lookup mismatch' , 'Not found in lookup table' , 'Identity match' , 'Identity mismatch']\n",
    "test_counts = dict.fromkeys(test_outcomes , 0)\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "### Training: read training data and populate lemma counters\n",
    "\n",
    "train_data = open (train_file , 'r')\n",
    "\n",
    "for line in train_data:\n",
    "    \n",
    "    # Tab character identifies lines containing tokens\n",
    "    if re.search ('\\t' , line):\n",
    "\n",
    "        # Tokens represented as tab-separated fields\n",
    "        field = line.strip().split('\\t')\n",
    "\n",
    "        # Word form in second field, lemma in third field\n",
    "        form = field[1]\n",
    "        lemma = field[2]\n",
    "        wordform_tokens += 1\n",
    "        \n",
    "        ######################################################\n",
    "        ### Insert code for populating the lemma counts  (coded below)  ###\n",
    "        ######################################################\n",
    "        \n",
    "        # Add unseen form to nested Dict.\n",
    "        if form not in lemma_count:\n",
    "            lemma_count[form] = {lemma : 1}\n",
    "        else:\n",
    "            # If form and lemma already exist in Dict.\n",
    "            if lemma in lemma_count[form]:\n",
    "                # Add count\n",
    "                lemma_count[form][lemma] = lemma_count[form][lemma] + 1\n",
    "            # If a form exist but a lemma don't\n",
    "            else:\n",
    "                lemma_count[form][lemma] = 1\n",
    "\n",
    "#print(dict(list(lemma_count.items())[0:10]))\n",
    "\n",
    "### Model building and training statistics\n",
    "\n",
    "#for form in lemma_count.keys():\n",
    "\n",
    "# Get key, value from lemma_count Dict. to each variable keys and values\n",
    "for keys, values in lemma_count.items():\n",
    "    \n",
    "    ######################################################\n",
    "    ### Insert code for building the lookup table (coded below)      ###\n",
    "    ######################################################\n",
    "    \n",
    "    # From values get max counted value and add it into new Dict. in form of {key : value}\n",
    "    lemma_max[keys] = max(lemma_count[keys].items(), key = operator.itemgetter(1))[0]\n",
    "    \n",
    "    ######################################################\n",
    "    ### Insert code for populating the training counts (coded below)###\n",
    "    ######################################################\n",
    "    \n",
    "    tokens = values.keys()\n",
    "    \n",
    "    if len(values) == 1:\n",
    "        unambiguous_types += 1\n",
    "        \n",
    "        for token in tokens:\n",
    "            unambiguous_tokens += lemma_count[keys][token]\n",
    "            \n",
    "            if keys == token:\n",
    "                identity_tokens += lemma_count[keys][token]\n",
    "    else:\n",
    "        ambiguous_types += 1\n",
    "        \n",
    "        for token in tokens:\n",
    "            ambiguous_tokens += lemma_count[keys][token]\n",
    "            if token == lemma_max[keys]:\n",
    "                ambiguous_most_common_tokens += lemma_count[keys][token]\n",
    "            \n",
    "            if keys == token:\n",
    "                identity_tokens += lemma_count[keys][token]\n",
    "                \n",
    "\n",
    "# print(dict(list(lemma_max.items())[0:10]))\n",
    "\n",
    "\n",
    "# Fill out the data in training_counts Dict.\n",
    "training_counts['Wordform types'] = len(lemma_count)\n",
    "training_counts['Wordform tokens'] = wordform_tokens\n",
    "training_counts['Unambiguous types'] = unambiguous_types\n",
    "training_counts['Unambiguous tokens'] = unambiguous_tokens\n",
    "training_counts['Ambiguous types'] = ambiguous_types\n",
    "training_counts['Ambiguous tokens'] = ambiguous_tokens\n",
    "training_counts['Ambiguous most common tokens'] = ambiguous_most_common_tokens\n",
    "training_counts['Identity tokens'] = identity_tokens\n",
    "\n",
    "accuracies['Expected lookup'] = float((unambiguous_tokens + ambiguous_most_common_tokens) / wordform_tokens)\n",
    "accuracies['Expected identity'] = float(identity_tokens / wordform_tokens)\n",
    "\n",
    "### Testing: read test data, and compare lemmatizer output to actual lemma\n",
    "    \n",
    "test_data = open (test_file , 'r')\n",
    "test_tokens, lookup_tokens, identity_tokens = 0, 0, 0\n",
    "lookup_match, lookup_mismatch, identity_match, identity_mismatch = 0, 0, 0, 0\n",
    "\n",
    "for line in test_data:\n",
    "\n",
    "    # Tab character identifies lines containing tokens\n",
    "    if re.search ('\\t' , line):\n",
    "\n",
    "        # Tokens represented as tab-separated fields\n",
    "        field = line.strip().split('\\t')\n",
    "\n",
    "        # Word form in second field, lemma in third field\n",
    "        form = field[1]\n",
    "        lemma = field[2]\n",
    "        test_tokens += 1\n",
    "        \n",
    "        ######################################################\n",
    "        ### Insert code for populating the test counts     ###\n",
    "        ######################################################\n",
    "        \n",
    "        # Check if test form is in lookup table\n",
    "        if form in lemma_max:\n",
    "            # Check if test lemma matches the one in lookup table\n",
    "            if lemma == lemma_max[form]:\n",
    "                # Add 1 to lookup_match \n",
    "                lookup_match += 1\n",
    "            else:\n",
    "                # Add 1 to lookup_mismatch \n",
    "                lookup_mismatch += 1\n",
    "        \n",
    "        # Check if form is out of vocabulary\n",
    "        else:\n",
    "            # Check if it is identify matching\n",
    "            if form == lemma:\n",
    "                # Add 1 to identity_match\n",
    "                identity_match += 1\n",
    "            else:\n",
    "                # Add 1 to identity_mismatch\n",
    "                identity_mismatch += 1\n",
    "\n",
    "test_counts['Total test items'] = test_tokens\n",
    "test_counts['Found in lookup table'] = lookup_match + lookup_mismatch\n",
    "test_counts['Lookup match'] = lookup_match\n",
    "test_counts['Lookup mismatch'] = lookup_mismatch\n",
    "test_counts['Not found in lookup table'] = identity_match + identity_mismatch\n",
    "test_counts['Identity match'] = identity_match\n",
    "test_counts['Identity mismatch'] = identity_mismatch\n",
    "\n",
    "accuracies['Lookup'] = float(lookup_match / (lookup_match + lookup_mismatch))\n",
    "accuracies['Identity'] = float(identity_match / (identity_match + identity_mismatch))\n",
    "accuracies['Overall'] = float((lookup_match + identity_match) / (lookup_match + lookup_mismatch + identity_match + identity_mismatch))\n",
    "\n",
    "### Report training statistics and test results\n",
    "                \n",
    "output = open ('lookup-output.txt' , 'w')\n",
    "\n",
    "output.write ('Training statistics\\n')\n",
    "\n",
    "for stat in training_stats:\n",
    "    output.write (stat + ': ' + str(training_counts[stat]) + '\\n')\n",
    "\n",
    "for model in ['Expected lookup' , 'Expected identity']:\n",
    "    output.write (model + ' accuracy: ' + str(accuracies[model]) + '\\n')\n",
    "\n",
    "output.write ('Test results\\n')\n",
    " \n",
    "for outcome in test_outcomes:\n",
    "    output.write (outcome + ': ' + str(test_counts[outcome]) + '\\n')\n",
    "\n",
    "for model in ['Lookup' , 'Identity' , 'Overall']:\n",
    "    output.write (model + ' accuracy: ' + str(accuracies[model]) + '\\n')\n",
    "\n",
    "output.close\n",
    "\n",
    "print(training_counts)\n",
    "print(test_counts)\n",
    "\n",
    "for model in ['Expected lookup' , 'Expected identity']:\n",
    "    print(model + ' accuracy: ' + str(accuracies[model]))\n",
    "for model in ['Lookup' , 'Identity' , 'Overall']:\n",
    "    print(model + ' accuracy: ' + str(accuracies[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = 0\n",
    "#people = {1: {'name': 'John', 'age': 27, 'sex': 'Male'},\n",
    "#          2: {'name': 'Marie', 'age': 22, 'sex': 'Female'},\n",
    "#          3: {'name': 'Luna', 'age': 24, 'sex': 'Female', 'married': 'No'}}\n",
    "people = {1: {'name': 22, 'age': 27, 'sex': 21}}\n",
    "\n",
    "for ppls, infos in people.items():\n",
    "    keys = infos.keys()\n",
    "    for key in keys:\n",
    "        ages += people[ppls][key]\n",
    "        print(people[ppls][key])\n",
    "\n",
    "print(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {'John': {'name': 'John'},\n",
    "          'James': {'name': 'Marie', 'temp': 'temporary'},\n",
    "          'Smith': {'name': 'Luna'}}\n",
    "\n",
    "for ppls, infos in people.items():\n",
    "    if len(infos) == 1:\n",
    "        print(people[ppls])\n",
    "        print('len = 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {1: {'name': 'John', 'age': '27', 'sex': 'Male'},\n",
    "          2: {'name': 'Marie', 'age': '22', 'sex': 'Female'},\n",
    "          3: {'name': 'Luna', 'age': '24', 'sex': 'Female', 'married': 'No'}}\n",
    "\n",
    "for i in range(1, 3):\n",
    "    if i in people:\n",
    "        print(people[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = {'John': {'name': 'John'},\n",
    "          'James': {'name': 'Marie', 'temp': 'temporary'},\n",
    "          'Smith': {'name': 'Luna'}}\n",
    "\n",
    "if 'Smith' in people:\n",
    "    print('Smith')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

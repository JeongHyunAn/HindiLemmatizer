{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Wordform types': 16879, 'Wordform tokens': 281057, 'Unambiguous types': 16465, 'Unambiguous tokens': 196204, 'Ambiguous types': 414, 'Ambiguous tokens': 84853, 'Ambiguous most common tokens': 75667, 'Identity tokens': 0}\n"
     ]
    }
   ],
   "source": [
    "### This program is a very simple lemmatizer, which learns a\n",
    "### lemmatization function from an annotated corpus. The function is\n",
    "### so basic I wouldn't even consider it machine learning: it's\n",
    "### basically just a big lookup table, which maps every word form\n",
    "### attested in the training data to the most common lemma associated\n",
    "### with that form. At test time, the program checks if a form is in\n",
    "### the lookup table, and if so, it gives the associated lemma; if the\n",
    "### form is not in the lookup table, it gives the form itself as the\n",
    "### lemma (identity mapping).\n",
    "\n",
    "### The program performs training and testing in one run: it reads the\n",
    "### training data, learns the lookup table and keeps it in memory,\n",
    "### then reads the test data, runs the testing, and reports the\n",
    "### results.\n",
    "\n",
    "### The program takes two command line arguments, which are the paths\n",
    "### to the training and test files. Both files are assumed to be\n",
    "### already tokenized, in Universal Dependencies format, that is: each\n",
    "### token on a separate line, each line consisting of fields separated\n",
    "### by tab characters, with word form in the second field, and lemma\n",
    "### in the third field. Tab characters are assumed to occur only in\n",
    "### lines corresponding to tokens; other lines are ignored.\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "### Global variables\n",
    "\n",
    "# Paths for data are read from command line\n",
    "#train_file = sys.argv[1]\n",
    "#test_file = sys.argv[2]\n",
    "train_file = './data/hi_hdtb-ud-train.conllu'\n",
    "test_file = './data/hi_hdtb-ud-test.conllu'\n",
    "\n",
    "# Counters for lemmas in the training data: word form -> lemma -> count\n",
    "lemma_count = {}\n",
    "duplicated_lemma_list, lemma_list = {}, {}\n",
    "duplicated_form_list, form_list = {}, {}\n",
    "\n",
    "# Lookup table learned from the training data: word form -> lemma\n",
    "lemma_max = {}\n",
    "\n",
    "# Variables for reporting results\n",
    "training_stats = ['Wordform types' , 'Wordform tokens' , 'Unambiguous types' , 'Unambiguous tokens' , 'Ambiguous types' , 'Ambiguous tokens' , 'Ambiguous most common tokens' , 'Identity tokens']\n",
    "training_counts = dict.fromkeys(training_stats , 0)\n",
    "\n",
    "test_outcomes = ['Total test items' , 'Found in lookup table' , 'Lookup match' , 'Lookup mismatch' , 'Not found in lookup table' , 'Identity match' , 'Identity mismatch']\n",
    "test_counts = dict.fromkeys(test_outcomes , 0)\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "### Training: read training data and populate lemma counters\n",
    "\n",
    "train_data = open (train_file , 'r')\n",
    "#test_data = open (test_file , 'r')\n",
    "\n",
    "for line in train_data:\n",
    "    \n",
    "    # Tab character identifies lines containing tokens\n",
    "    if re.search ('\\t' , line):\n",
    "\n",
    "        # Tokens represented as tab-separated fields\n",
    "        field = line.strip().split('\\t')\n",
    "\n",
    "        # Word form in second field, lemma in third field\n",
    "        form = field[1]\n",
    "        lemma = field[2]\n",
    "        \n",
    "        # Form -> lemma\n",
    "        if form not in duplicated_form_list: # Add new form\n",
    "            duplicated_form_list[form] = [lemma]\n",
    "        else:\n",
    "            duplicated_form_list[form].append(lemma)\n",
    "        \n",
    "        # Lemma -> form\n",
    "        if lemma not in duplicated_lemma_list:  # Add new lemma\n",
    "            duplicated_lemma_list[lemma] = [form]\n",
    "        else:\n",
    "            duplicated_lemma_list[lemma].append(form)\n",
    "            \n",
    "        # Form -> lemma (no duplicates)\n",
    "        if form not in form_list: # Add new form\n",
    "            form_list[form] = [lemma]\n",
    "        else:\n",
    "            if lemma not in form_list[form]: # For a lemma if form -> lemma exist do not add\n",
    "                form_list[form].append(lemma)\n",
    "        \n",
    "        # Lemma -> form (no duplicates)\n",
    "        if lemma not in lemma_list: # Add new lemma\n",
    "            lemma_list[lemma] = [form]\n",
    "        else:\n",
    "            if form not in lemma_list[lemma]: # For a form if lemma -> form exist do not add\n",
    "                lemma_list[lemma].append(form)\n",
    "            \n",
    "plural_types = [key for key, value in form_list.items() if len(value) > 1] # Get forms which have plural tokens\n",
    "singular_types = [key for key, value in form_list.items() if len(value) == 1] # Get forms which have singular tokens\n",
    "\n",
    "all_tokens = 0\n",
    "for value in duplicated_lemma_list.values():\n",
    "    all_tokens = all_tokens + len(value)\n",
    "\n",
    "singular_tokens = 0\n",
    "for types in singular_types:\n",
    "    if types in duplicated_form_list:\n",
    "        singular_tokens = singular_tokens + len(duplicated_form_list[types])\n",
    "\n",
    "plural_tokens = 0\n",
    "duplicated_ambiguous_tokens = {}\n",
    "ambiguous_tokens = {}\n",
    "for types in plural_types:\n",
    "    if types in duplicated_form_list:\n",
    "        plural_tokens = plural_tokens + len(duplicated_form_list[types])\n",
    "        duplicated_ambiguous_tokens[types] = duplicated_form_list[types]\n",
    "        ambiguous_tokens[types] = form_list[types]\n",
    "\n",
    "most_common_tokens = {}\n",
    "for key, values in duplicated_form_list.items():\n",
    "    token_frequency = {}\n",
    "    for value in values:\n",
    "        if value not in token_frequency:\n",
    "            token_frequency[value] = 0\n",
    "        else:\n",
    "            token_frequency[value] += 1\n",
    "    most_common_tokens[key] = max(token_frequency, key = token_frequency.get)\n",
    "\n",
    "ambiguous_most_common = {}\n",
    "common_token_count = 0\n",
    "for key, value in most_common_tokens.items():\n",
    "    for keys in duplicated_ambiguous_tokens.keys():\n",
    "        if key == keys:\n",
    "            values = duplicated_ambiguous_tokens[keys]\n",
    "            #print()\n",
    "            for i in range(len(values)):\n",
    "                if value == values[i]:\n",
    "                    #print(str(len(values)) + '중 ' + str(i + 1) + '번째 비교 : '+ key + ' : ' + value + ' / ' + values[i] + 'MATCH!!!!   token : ' + value)\n",
    "                    ambiguous_most_common[key] = value\n",
    "                    common_token_count = common_token_count + 1\n",
    "                    #print(ambiguous_most_common)\n",
    "                #print(str(len(values)) + '중 ' + str(i + 1) + '번째 비교 : '+ key + ' : ' + value + ' / ' + values[i])\n",
    "\n",
    "training_counts['Wordform types'] = len(form_list.keys())\n",
    "training_counts['Wordform tokens'] = all_tokens\n",
    "training_counts['Unambiguous types'] = len(form_list.keys()) - len(plural_types)\n",
    "training_counts['Unambiguous tokens'] = singular_tokens\n",
    "training_counts['Ambiguous types'] = len(plural_types)\n",
    "training_counts['Ambiguous tokens'] = plural_tokens\n",
    "training_counts['Ambiguous most common tokens'] = common_token_count\n",
    "\n",
    "\n",
    "        ######################################################\n",
    "        ### Insert code for populating the lemma counts    ###\n",
    "        ######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rev_dict = {}\n",
    "\n",
    "for key, value in lemma_count.items():\n",
    "    if value not in rev_dict:\n",
    "        rev_dict[value] = [key]\n",
    "    else:\n",
    "        rev_dict[value].append(key)\n",
    "\n",
    "print(len(rev_dict))\n",
    "\n",
    "result = [key for key, values in rev_dict.items() if len(values) > 1]\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a' : 10, 'b' : [20, 30]}\n",
    "print(len(a))\n",
    "print(len(a.keys()))\n",
    "print(len(a.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a' : 10, 'b' : 20, 'c' : 30, 'd' : 40}\n",
    "b = {'a' : [10, 20, 30]}\n",
    "\n",
    "for key, value in a.items():\n",
    "    for keys in b.keys():\n",
    "        if key == keys:\n",
    "            for i in range(len(b[keys])):\n",
    "                if value == values[i]:\n",
    "                    print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
